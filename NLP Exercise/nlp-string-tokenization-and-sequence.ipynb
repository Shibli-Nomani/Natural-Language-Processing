{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <p style=\"background-color:#746AB0;font-family:newtimeroman;color:#EFF0F7;font-size:150%;text-align:center;border-radius:10px 10px;\">Natural Language Processing (String Tokenization)</p>","metadata":{}},{"cell_type":"markdown","source":"**Natural Language Processing(NLP):** It's a field of computer science and more specially Artificial Intellegency. It's a process to improve the ability of machine to mimic like Human in terms of **Text and Speech** \n\nNLP has two parts. One is for Understanding and another one is for Genaration.\n\n**Tokenization:** Tokenization is a part of **Text Understanding and Preprocessing**. It makes the unstructure data/text into a chunk of information as a **Vector**. Later it will provide text to numerical representation.","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:#746AB0;font-family:newtimeroman;color:#EFF0F7;font-size:150%;text-align:center;border-radius:10px 10px;\">Methods of Tokenizing</p>\n\nhttps://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:#746AB0;font-family:newtimeroman;color:#EFF0F7;font-size:150%;text-align:center;border-radius:10px 10px;\">Import Libaries</p>","metadata":{}},{"cell_type":"markdown","source":"**`keras.preprocessing.text.tokenizer`:** This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector ","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer #tokenize the string \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences #to maintain the sequence in the list\n","metadata":{"execution":{"iopub.status.busy":"2022-10-07T08:48:05.885208Z","iopub.execute_input":"2022-10-07T08:48:05.885666Z","iopub.status.idle":"2022-10-07T08:48:05.891903Z","shell.execute_reply.started":"2022-10-07T08:48:05.885629Z","shell.execute_reply":"2022-10-07T08:48:05.890355Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:#746AB0;font-family:newtimeroman;color:#EFF0F7;font-size:150%;text-align:center;border-radius:10px 10px;\">Text Preprocessing</p>","metadata":{}},{"cell_type":"markdown","source":"**For LIst of single sentence**","metadata":{}},{"cell_type":"code","source":"sentence = [\"I love anime\"]\n","metadata":{"execution":{"iopub.status.busy":"2022-10-07T09:53:44.594710Z","iopub.execute_input":"2022-10-07T09:53:44.595200Z","iopub.status.idle":"2022-10-07T09:53:44.600794Z","shell.execute_reply.started":"2022-10-07T09:53:44.595163Z","shell.execute_reply":"2022-10-07T09:53:44.599124Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=50, oov_token =\"<UNK>\")#The word index will not be affected by the nums of word variable.\n#instead, it will have an impact on the text sequence \n#it will only be trained on num words minus one words, and the rest of the words won't be considered\n#if you don't use a token for out of vocabulary (oov token) words, the word won't be encoded, and it will be skipped in the sequence\n","metadata":{"execution":{"iopub.status.busy":"2022-10-07T09:53:59.449125Z","iopub.execute_input":"2022-10-07T09:53:59.450308Z","iopub.status.idle":"2022-10-07T09:53:59.456429Z","shell.execute_reply.started":"2022-10-07T09:53:59.450261Z","shell.execute_reply":"2022-10-07T09:53:59.455261Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"tokenizer.fit_on_texts(sentence)","metadata":{"execution":{"iopub.status.busy":"2022-10-07T09:54:03.401969Z","iopub.execute_input":"2022-10-07T09:54:03.402416Z","iopub.status.idle":"2022-10-07T09:54:03.408225Z","shell.execute_reply.started":"2022-10-07T09:54:03.402379Z","shell.execute_reply":"2022-10-07T09:54:03.406628Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"index = tokenizer.word_index #indexing most frequent words appears in sentences\n\nindex","metadata":{"execution":{"iopub.status.busy":"2022-10-07T09:54:11.054441Z","iopub.execute_input":"2022-10-07T09:54:11.054833Z","iopub.status.idle":"2022-10-07T09:54:11.062688Z","shell.execute_reply.started":"2022-10-07T09:54:11.054802Z","shell.execute_reply":"2022-10-07T09:54:11.061485Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"{'<UNK>': 1, 'i': 2, 'love': 3, 'anime': 4}"},"metadata":{}}]},{"cell_type":"markdown","source":"**For multiple sentences in a list**","metadata":{}},{"cell_type":"code","source":"sentences = [\n    \"I love anime\",\n    \"Anime is the best\",\n    \"OnePiece is my favourite Anime\",\n    \"OnePiece is amazing and is loved by every Japanese. Thus it is best in the world\"\n]","metadata":{"execution":{"iopub.status.busy":"2022-10-07T09:54:40.639911Z","iopub.execute_input":"2022-10-07T09:54:40.640317Z","iopub.status.idle":"2022-10-07T09:54:40.646013Z","shell.execute_reply.started":"2022-10-07T09:54:40.640285Z","shell.execute_reply":"2022-10-07T09:54:40.644715Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"**`oov_token =\"<UNK>\"`** Oov tokens are out of vocabulary tokens used to replace unknown words. Words that are less frequent are replaced with the **`<unk>`** symbol.","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=50, oov_token =\"<UNK>\") #The word index will not be affected by the nums of word variable.\n#instead, it will have an impact on the text sequence \n#it will only be trained on num words minus one words, and the rest of the words won't be considered\n#if you don't use a token for out of vocabulary (oov token) words, the word won't be encoded, and it will be skipped in the sequence","metadata":{"execution":{"iopub.status.busy":"2022-10-07T09:54:44.581727Z","iopub.execute_input":"2022-10-07T09:54:44.582159Z","iopub.status.idle":"2022-10-07T09:54:44.587249Z","shell.execute_reply.started":"2022-10-07T09:54:44.582116Z","shell.execute_reply":"2022-10-07T09:54:44.586346Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"tokenizer.fit_on_texts(sentences)","metadata":{"execution":{"iopub.status.busy":"2022-10-07T09:54:47.558958Z","iopub.execute_input":"2022-10-07T09:54:47.559981Z","iopub.status.idle":"2022-10-07T09:54:47.566120Z","shell.execute_reply.started":"2022-10-07T09:54:47.559936Z","shell.execute_reply":"2022-10-07T09:54:47.564668Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"index = tokenizer.word_index #indexing most frequent words appears in sentences\n\nindex","metadata":{"execution":{"iopub.status.busy":"2022-10-07T09:54:48.911371Z","iopub.execute_input":"2022-10-07T09:54:48.911772Z","iopub.status.idle":"2022-10-07T09:54:48.919951Z","shell.execute_reply.started":"2022-10-07T09:54:48.911740Z","shell.execute_reply":"2022-10-07T09:54:48.918544Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"{'<UNK>': 1,\n 'is': 2,\n 'anime': 3,\n 'the': 4,\n 'best': 5,\n 'onepiece': 6,\n 'i': 7,\n 'love': 8,\n 'my': 9,\n 'favourite': 10,\n 'amazing': 11,\n 'and': 12,\n 'loved': 13,\n 'by': 14,\n 'every': 15,\n 'japanese': 16,\n 'thus': 17,\n 'it': 18,\n 'in': 19,\n 'world': 20}"},"metadata":{}}]},{"cell_type":"markdown","source":"**`texts_to_sequences`** is maintaining the seq of sentences as per list after tokenizing","metadata":{}},{"cell_type":"code","source":"seq = tokenizer.texts_to_sequences(sentences) #used for the encoding of a string of phrases\nseq","metadata":{"execution":{"iopub.status.busy":"2022-10-07T09:54:51.346225Z","iopub.execute_input":"2022-10-07T09:54:51.346940Z","iopub.status.idle":"2022-10-07T09:54:51.355659Z","shell.execute_reply.started":"2022-10-07T09:54:51.346902Z","shell.execute_reply":"2022-10-07T09:54:51.354381Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"[[7, 8, 3],\n [3, 2, 4, 5],\n [6, 2, 9, 10, 3],\n [6, 2, 11, 12, 2, 13, 14, 15, 16, 17, 18, 2, 5, 19, 4, 20]]"},"metadata":{}}]},{"cell_type":"code","source":"new = ['I am busy in my work']\nnewSeq = tokenizer.texts_to_sequences(new)\nnewSeq","metadata":{"execution":{"iopub.status.busy":"2022-10-07T09:54:52.802231Z","iopub.execute_input":"2022-10-07T09:54:52.802658Z","iopub.status.idle":"2022-10-07T09:54:52.809815Z","shell.execute_reply.started":"2022-10-07T09:54:52.802624Z","shell.execute_reply":"2022-10-07T09:54:52.808625Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"[[7, 1, 1, 19, 9, 1]]"},"metadata":{}}]},{"cell_type":"code","source":"test = [\"Let's play the football\",\n        \"I am buying football\",\n        \"Football world cup is very near\"]\ntestSeq = tokenizer.texts_to_sequences(test)\nprint (\"indexing of model\", '\\n', index)\nprint('----------------------------------------------------------------------------------------------------------------')\nprint(\"sequence list of test data\", '\\n', testSeq)","metadata":{"execution":{"iopub.status.busy":"2022-10-07T09:54:54.126992Z","iopub.execute_input":"2022-10-07T09:54:54.127415Z","iopub.status.idle":"2022-10-07T09:54:54.135281Z","shell.execute_reply.started":"2022-10-07T09:54:54.127381Z","shell.execute_reply":"2022-10-07T09:54:54.134102Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"indexing of model \n {'<UNK>': 1, 'is': 2, 'anime': 3, 'the': 4, 'best': 5, 'onepiece': 6, 'i': 7, 'love': 8, 'my': 9, 'favourite': 10, 'amazing': 11, 'and': 12, 'loved': 13, 'by': 14, 'every': 15, 'japanese': 16, 'thus': 17, 'it': 18, 'in': 19, 'world': 20}\n----------------------------------------------------------------------------------------------------------------\nsequence list of test data \n [[1, 1, 4, 1], [7, 1, 1, 1], [1, 20, 1, 2, 1, 1]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**`pad_sequences`** array representation of a list of sentences after tokenizing by model. **`maxlen`** is used to controll the array column","metadata":{}},{"cell_type":"code","source":"padded = pad_sequences (testSeq, truncating ='post', maxlen = 6 )\npadded","metadata":{"execution":{"iopub.status.busy":"2022-10-07T09:54:57.383249Z","iopub.execute_input":"2022-10-07T09:54:57.383685Z","iopub.status.idle":"2022-10-07T09:54:57.392568Z","shell.execute_reply.started":"2022-10-07T09:54:57.383647Z","shell.execute_reply":"2022-10-07T09:54:57.391221Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"array([[ 0,  0,  1,  1,  4,  1],\n       [ 0,  0,  7,  1,  1,  1],\n       [ 1, 20,  1,  2,  1,  1]], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"# <p style=\"background-color:#746AB0;font-family:newtimeroman;color:#EFF0F7;font-size:150%;text-align:center;border-radius:10px 10px;\">Summary</p>\nTokenizing is based on frequency of words in a list of sentences. Oov is used to tokenized the unknown word. indexing value is represented as key value pair by dictoinary. texts_to_sequences is maintaining the sentences sequence in the list after tokenizing and pad_sequences provide us array representation as an output","metadata":{}}]}